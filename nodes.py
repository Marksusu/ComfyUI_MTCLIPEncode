from transformers import MarianMTModel, MarianTokenizer
import re

marian_list = [
    "opus-mt-zh-en",
    "opus-mt-ru-en",
    "opus-mt-th-en",
]
# https://huggingface.co/Helsinki-NLP åœ¨è¿™ä¸ªåœ°å€é‡Œå¯ä»¥æ‰¾åˆ°æ›´å¤šè¯­ç§çš„æ¨¡å‹ï¼Œæ·»åŠ åˆ°å¦‚ä¸Šåˆ—è¡¨åï¼Œä¸‹æ¬¡ä½¿ç”¨æ—¶ä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹

class MTCLIPEncode:
    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "clip": ("CLIP", {}),
                "checkpoint": (marian_list, {"multiline": False,"default": "opus-mt-zh-en"}),
                "text": ("STRING", {"multiline": True,"default": "prefix | translate_part | suffix"}),
            }
        }

    def is_valid_translate_part(self, text):
        # ç®€å•æ£€æŸ¥æ–‡æœ¬æ˜¯å¦åŒ…å«è‡³å°‘ä¸€ä¸ªæ±‰å­—æˆ–ä¸€ä¸ªé•¿åº¦å¤§äº2çš„éè‹±æ–‡å•è¯
        return bool(re.search(r'[\u4e00-\u9fff]|\b\w{3,}\b', text))

    def mtencode(self, clip, checkpoint, text):

        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ä¿®å‰ªé¦–å°¾å¯èƒ½å­˜åœ¨çš„ä¸€ä¸ªæˆ–å¤šä¸ª `,`ã€[ç©ºæ ¼]
        text = re.sub(r'^[\s,]+|[\s,]+$', '', text)

        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åˆ†ç¦»å‡ºéœ€è¦ç¿»è¯‘å’Œä¸éœ€è¦ç¿»è¯‘çš„éƒ¨åˆ†
        pattern = r"([^|]*)\|([^|]*)\|([^|]*)"
        match = re.match(pattern, text)

        if match:
            prefix = match.group(1).strip()
            translate_part = match.group(2).strip()
            suffix = match.group(3).strip()

            # ä¿®å‰ªä¸‰ä¸ªéƒ¨åˆ†çš„æ–‡æœ¬ï¼Œé¦–å°¾å¯èƒ½å­˜åœ¨çš„ä¸€ä¸ªæˆ–å¤šä¸ª`|`ã€`,`ã€[ç©ºæ ¼]
            prefix = re.sub(r'^[\s|,]+|[\s|,]+$', '', prefix)
            translate_part = re.sub(r'^[\s|,]+|[\s|,]+$', '', translate_part)
            suffix = re.sub(r'^[\s|,]+|[\s|,]+$', '', suffix)

            # ç¡®ä¿ translate_part æœ‰æ•ˆ
            if not self.is_valid_translate_part(translate_part):
                prompt_text = f"{prefix}, {suffix}"
            else:
                model_name = 'Helsinki-NLP/' + checkpoint
                tokenizer = MarianTokenizer.from_pretrained(model_name)
                model = MarianMTModel.from_pretrained(model_name)

                translated = model.generate(**tokenizer(translate_part, return_tensors="pt", padding=True))
                translated_text = tokenizer.decode(translated[0], skip_special_tokens=True)

                # ä¿®å‰ª translated_text é¦–å°¾å¯èƒ½å­˜åœ¨çš„ç©ºæ ¼ã€é€—å·å’Œå¥å·
                translated_text = re.sub(r'^[\s,.]+|[\s,.]+$', '', translated_text)

                # åˆå¹¶ä¸éœ€è¦ç¿»è¯‘çš„éƒ¨åˆ†å’Œç¿»è¯‘åçš„éƒ¨åˆ†
                if prefix and suffix:
                    prompt_text = f"{prefix}, {translated_text}, {suffix}"
                elif prefix:
                    prompt_text = f"{prefix}, {translated_text}"
                elif suffix:
                    prompt_text = f"{translated_text}, {suffix}"
                else:
                    prompt_text = translated_text

                # ANSI è½¬ä¹‰åºåˆ—ç”¨äºé¢œè‰²è¾“å‡º
                color_prefix = f"\033[94m{prefix}\033[0m" if prefix else ""
                color_translate_part = f"\033[92m{translate_part}\033[0m"
                color_translated_text = f"\033[92m{translated_text}\033[0m"
                color_suffix = f"\033[94m{suffix}\033[0m" if suffix else ""

                # æ‰“å°æ ¼å¼åŒ–è¾“å‡º
                print(f"ã€€ã€€ã€€ã€€ã€€ã€€ğŸ«ã€€ğŸ«ã€€ğŸ«ã€€ğŸ«ã€€ğŸ«ã€€ğŸ«")
                if prefix and suffix:
                    print(f"ã€€ã€€ã€€ã€€{color_prefix}, {color_translate_part}, {color_suffix}")
                elif prefix:
                    print(f"ã€€ã€€ã€€ã€€{color_prefix}, {color_translate_part}")
                elif suffix:
                    print(f"ã€€ã€€ã€€ã€€{color_translate_part}, {color_suffix}")
                else:
                    print(f"ã€€ã€€ã€€ã€€{color_translate_part}")

                if prefix and suffix:
                    print(f"ã€€ã€€ã€€ã€€{color_prefix}, {color_translated_text}, {color_suffix}")
                elif prefix:
                    print(f"ã€€ã€€ã€€ã€€{color_prefix}, {color_translated_text}")
                elif suffix:
                    print(f"ã€€ã€€ã€€ã€€{color_translated_text}, {color_suffix}")
                else:
                    print(f"ã€€ã€€ã€€ã€€{color_translated_text}")

        else:
            prompt_text = text.strip()  # å¦‚æœæ²¡æœ‰åŒ¹é…é¡¹ï¼Œä½¿ç”¨åŸæ–‡æœ¬

        # æ‰“å°æ ¼å¼åŒ–è¾“å‡º
        print(f"ã€€ã€€ã€€ã€€{prompt_text}")

        tokens = clip.tokenize(prompt_text)
        cond, pooled = clip.encode_from_tokens(tokens, return_pooled=True)

        return ([[cond, {"pooled_output": pooled}]], prompt_text)

    RETURN_TYPES = (
        "CONDITIONING",
        "STRING",
    )
    FUNCTION = "mtencode"
    CATEGORY = "MTCLIPEncode"

NODE_CLASS_MAPPINGS = {
    "MTCLIPEncode": MTCLIPEncode,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "MTCLIPEncode": "MTCLIPEncode",
}
