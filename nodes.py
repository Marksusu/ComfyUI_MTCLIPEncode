from transformers import MarianMTModel, MarianTokenizer
import re
import json
import requests
from ollama import Client, Options

# å»è¿™ä¸ªåœ°å€ï¼š https://huggingface.co/Helsinki-NLP å¯ä»¥æ‰¾åˆ°æ›´å¤šè¯­ç§çš„æ¨¡å‹ï¼Œç”¨åŒæ ·çš„æ ¼å¼æ·»åŠ åˆ°å¦‚ä¸‹åˆ—è¡¨ï¼Œä»¥ä¾¿ä¸‹æ¬¡é€‰ç”¨ï¼Œç¿»è¯‘æ¨¡å‹ä¼šè¢«è‡ªåŠ¨ä¸‹è½½è‡³æœ¬åœ°
marian_list = [
    "opus-mt-zh-en",
    "opus-mt-ru-en",
    "opus-mt-th-en",
    "opus-mt-es-en",
    "opus-mt-fr-en",
    "opus-mt-ar-en",
    "opus-mt-hi-en",
    "opus-mt-bn-en",
    "opus-mt-pt-en",
    "opus-mt-ja-en",
    "opus-mt-de-en",
    "opus-mt-ko-en",
    "opus-mt-vi-en",
    "opus-mt-tr-en",
    "opus-mt-nl-en",
]

class MTCLIPEncode:
    def __init__(self):
        self.default_ollama_url = "http://127.0.0.1:11434"

    @classmethod
    def INPUT_TYPES(cls):
        return {
            "required": {
                "clip": ("CLIP", {}),
                "checkpoint": (marian_list, {"multiline": False, "default": "opus-mt-zh-en"}),
                "text": ("STRING", {"multiline": True, "default": " prefix | translate_part | suffix"}),
                "ollama_url": ("STRING", {"default": "http://127.0.0.1:11434"}),
                "ollama_model": ("STRING", {"default": "mistral-nemo:latest"}),
            }
        }

    # ç¡®ä¿åŒç«–çº¿ `||` ä¸­é—´çš„å†…å®¹ä¸ºæ±‰å­—æˆ–ä¸å°‘äºä¸‰ä¸ªå­—æ¯çš„éè‹±æ–‡å•è¯
    def is_valid_translate_part(self, text):
        return bool(re.search(r'[\u4e00-\u9fff]|\b\w{3,}\b', text))

    def optimize_with_ollama(self, text, url, model):
        client = Client(host=url)

        prompt = f"""
You are a talented art director with a deep understanding of visual arts. You excel at transforming simple text descriptions into vivid, imaginative images. Now, you will be playing the role of a prompt engineer, providing precise and creative prompts for the Stable Diffusion model.

Please convert the following user input text: {text}, into a detailed and vivid prompt to inspire Stable Diffusion to generate a captivating image.

**Focus on:**
* **Clarity and precision:** Use clear and concise language that Stable Diffusion can easily understand.
* **Visual storytelling:** Create compelling narratives through your descriptions.
* **Artistic techniques:** Employ a variety of artistic techniques like lighting, composition, and perspective.
* **Style and atmosphere:** Convey the desired mood and style using vivid language and metaphors.

**Consider incorporating:**
* **Specific art styles:** e.g., impressionism, surrealism, cyberpunk
* **Cinematographic techniques:** e.g., shallow depth of field, wide-angle shot
* **Descriptive adjectives:** e.g., ethereal, luminous, brooding
* **Compositional elements:** e.g., rule of thirds, golden ratio

**Example:**
* **User input:** A lonely robot sitting on a distant planet
* **Your prompt:** # solitary android, desolate alien planet, distant star, panoramic view, sci-fi aesthetic, melancholic atmosphere, cinematic lighting #

**Remember:** Your goal is to create a prompt that inspires Stable Diffusion to generate a visually stunning and emotionally resonant image.

**Please enclose your prompt within two `#` symbols.**
"""

        response = client.generate(model=model, prompt=prompt)
        return response['response']

    def mtencode(self, clip, checkpoint, text, ollama_url, ollama_model):

        print(f"ã€€ã€€ã€€ã€€ã€€ã€€ğŸ«ã€€ğŸ«ã€€ğŸ«ã€€ğŸ«ã€€ğŸ«ã€€ğŸ«")    # æ‰“å°åˆ†éš”çº¿

        pattern = r"([^|]*)\|([^|]*)\|([^|]*)"    # éœ€è¦è°ƒç”¨ç¿»è¯‘æ¨¡å‹çš„æç¤ºè¯åŸºæœ¬æ ¼å¼
        match = re.match(pattern, text)    # åˆ¤æ–­æ˜¯å¦éœ€è¦è°ƒç”¨ç¿»è¯‘æ¨¡å‹

        # è‹¥è¾“å…¥ text ä¸­ï¼Œæ²¡æœ‰`||`(æ²¡æœ‰ç¿»è¯‘è¯·æ±‚)ï¼Œåˆ¤æ–­å¹å·çš„æ•°é‡å’Œä½ç½®
        ollama_for_original = text.count('!') == 1 and '|' not in text  # å¯¹åº”çš„è¾“å…¥æ ¼å¼ä¸ºï¼š"English words English words ! English words"ï¼Œå¹å· `!` ä¸ºä¸€ä¸ªï¼Œå¯ä»¥åœ¨ä»»æ„ä½ç½®
        ollama_for_part_of_original = text.count('!') == 2 and '|' not in text  # å¯¹åº”æ ¼å¼ä¸ºï¼š"English prefix ! English words for Ollama refine ! English suffix"

        # æœ‰`||`ï¼Œè¯´æ˜æœ‰ç¿»è¯‘è¯·æ±‚ï¼Œè¿›è¡Œç¿»è¯‘å¤„ç†
        if match:
            prefix, translate_part, suffix = [re.sub(r'^[\s|,]+|[\s|,]+$', '', part.strip()) for part in match.groups()]    # è¢« `||` åŒç«–çº¿åˆ†éš”çš„ä¸‰ä¸ªéƒ¨åˆ†
            print(f"ã€€ã€€ã€€ã€€\033[94m{prefix}\033[0m, \033[0;1m|\033[0m \033[92m{translate_part}\033[0m, \033[0;1m|\033[0m \033[94m{suffix}\033[0m")    # æ‰“å°ç”¨æˆ·çš„åŸå§‹è¾“å…¥ï¼ˆæœ‰`|`ï¼‰ï¼Œå¹¶ç”¨é¢œè‰²åŒºåˆ†ä¸‰ä¸ªéƒ¨åˆ†

            # ollama_in_translate_part å’Œ ollama_for_all_3_parts çš„åˆ¤æ–­åœ¨è§£æå‡º prefixã€translate_part å’Œ suffix ä¹‹å
            ollama_in_translate_part = '!' in translate_part  # åˆ¤æ–­å¹å·æ˜¯å¦åœ¨ç¿»è¯‘éƒ¨åˆ†
            ollama_for_all_3_parts = (
                not ollama_in_translate_part and (
                   '!' in prefix or '!' in suffix  # åˆ¤æ–­å¹å·æ˜¯å¦åœ¨å‰ç¼€æˆ–åç¼€ä¸­
                )
            )

            if not self.is_valid_translate_part(translate_part):
                prompt_text = f"{prefix}, {suffix}"
            else:
                model_name = 'Helsinki-NLP/' + checkpoint
                tokenizer = MarianTokenizer.from_pretrained(model_name)
                model = MarianMTModel.from_pretrained(model_name)

                if ollama_in_translate_part:

                    translate_part = translate_part.replace('!', '')  # å»æ‰ `!`
                    translated = model.generate(**tokenizer(translate_part, return_tensors="pt", padding=True))
                    translated_text = re.sub(r'^[\s,.]+|[\s,.]+$', '', tokenizer.decode(translated[0], skip_special_tokens=True))
                    print(f"ã€€ã€€ã€€ã€€\033[94m{prefix}\033[0m, \033[92m{translated_text}\033[0m, \033[94m{suffix}\033[0m")    # æ‰“å°è¯‘æ–‡ï¼Œå¹¶ä¸åŸå§‹çš„å‰ã€åç¼€åˆå¹¶

                    try:
                        translated_text = self.optimize_with_ollama(translated_text, ollama_url, ollama_model)
                        match = re.search(r'#(.*?)#', translated_text)
                        if match:
                            translated_text = match.group(1)
                            translated_text = re.sub(r'^[\s|,]+|[\s|,]+$', '', translated_text.strip())

                        prompt_text = ", ".join(filter(None, [prefix, translated_text, suffix]))
                        print(f"ã€€ã€€ã€€ã€€\033[94m{prefix}\033[0m, \033[90;1m{translated_text}\033[0m, \033[94m{suffix}\033[0m")    # æ‰“å° Ollama å¯¹è¯‘æ–‡çš„å¤„ç†ç»“æœï¼Œå¹¶ä¸åŸå§‹çš„å‰ã€åç¼€åˆå¹¶
                    except Exception as e:
                        print(f"Failed to optimize prompt with Ollama in translated text: {e}")

                else:
                    translated = model.generate(**tokenizer(translate_part, return_tensors="pt", padding=True))
                    translated_text = re.sub(r'^[\s,.]+|[\s,.]+$', '', tokenizer.decode(translated[0], skip_special_tokens=True))

                    prompt_text = ", ".join(filter(None, [prefix, translated_text, suffix]))
                    print(f"ã€€ã€€ã€€ã€€\033[94m{prefix}\033[0m, \033[92m{translated_text}\033[0m, \033[94m{suffix}\033[0m")    # æ‰“å°è¯‘æ–‡ï¼Œä¿ç•™åŸå§‹å‰ã€åç¼€ä¸å˜ï¼Œå¹¶å®Œæˆåˆå¹¶

                    if ollama_for_all_3_parts:
                        prompt_text = prompt_text.replace('!', '')  # å»æ‰ `!`
                        try:
                            prompt_text = self.optimize_with_ollama(prompt_text, ollama_url, ollama_model)
                            match = re.search(r'#(.*?)#', prompt_text)
                            if match:
                                prompt_text = match.group(1)
                                prompt_text = re.sub(r'^[\s|,]+|[\s|,]+$', '', prompt_text.strip())

                            print(f"ã€€ã€€ã€€ã€€\033[90;1m{prompt_text}\033[0m")    # æ‰“å° Ollama å¯¹åˆå¹¶åçš„æç¤ºè¯ï¼ˆåŒ…æ‹¬å‰ã€åç¼€å’Œè¯‘æ–‡ï¼‰æ•´ä½“çš„å¤„ç†ç»“æœ
                        except Exception as e:
                            print(f"Failed to optimize prompt with Ollama in all 3 parts: {e}")
            # å»æ‰å‰ç¼€ prefix ä¸ºç©ºæ—¶ï¼Œå¼€å¤´ä¼šå‡ºç°çš„`,`
            prompt_text = prompt_text.lstrip(',')

        else:
            prompt_text = text.strip()
            print(f"ã€€ã€€ã€€ã€€\033[94m{prompt_text}\033[0m")    # æ‰“å°æ— éœ€ç¿»è¯‘çš„åŸå§‹æç¤ºè¯

            if ollama_for_part_of_original:
                try:
                    # ä½¿ç”¨æ­£åˆ™æå– `!!` ä¹‹é—´çš„éƒ¨åˆ†ï¼Œå¹¶ç§»é™¤ `!!`
                    match = re.search(r'^(.*?)!(.*?)!(.*?)$', text)
                    if match:
                        prefix, ollama_part, suffix = [re.sub(r'^[\s|,]+|[\s|,]+$', '', part.strip()) for part in match.groups()]
#                        prefix, ollama_part, suffix = match.groups()
                        ollama_part = self.optimize_with_ollama(ollama_part.strip(), ollama_url, ollama_model)
                        ollama_match = re.search(r'#(.*?)#', ollama_part)
                        if ollama_match:
                            ollama_part = ollama_match.group(1)
                            ollama_part = re.sub(r'^[\s|,]+|[\s|,]+$', '', ollama_part.strip())

                        prompt_text = f"{prefix.strip()}, {ollama_part.strip()}, {suffix.strip()}"
                        print(f"ã€€ã€€ã€€ã€€\033[32m{prefix}\033[0m, \033[90;1m{ollama_part}\033[0m, \033[32m{suffix}\033[0m")    # æ‰“å° Ollama å¯¹æ— éœ€ç¿»è¯‘çš„åŸå§‹æç¤ºè¯çš„å¤„ç†ç»“æœï¼Œå¹¶ä¿ç•™åŸå§‹å‰ã€åç¼€
                except Exception as e:
                    print(f"Failed to optimize prompt with Ollama in part of original prompt: {e}")

            elif ollama_for_original:
                prompt_text = text.replace('!', '')  # å»æ‰ `!`
                try:
                    prompt_text = self.optimize_with_ollama(prompt_text, ollama_url, ollama_model)
                    match = re.search(r'#(.*?)#', prompt_text)
                    if match:
                        prompt_text = match.group(1)
                        prompt_text = re.sub(r'^[\s|,]+|[\s|,]+$', '', prompt_text.strip())

                    print(f"ã€€ã€€ã€€ã€€\033[90;1m{prompt_text}\033[0m")    # æ‰“å°æ— éœ€ç¿»è¯‘ä½†éœ€è¦ Ollama å¤„ç†çš„æç¤ºè¯
                except Exception as e:
                    print(f"Failed to optimize prompt with Ollama in original prompt: {e}")

            # å»æ‰å‰ç¼€ prefix ä¸ºç©ºæ—¶ï¼Œå¼€å¤´ä¼šå‡ºç°çš„`,`
            prompt_text = prompt_text.lstrip(',')

        # æœ€ç»ˆè¾“å‡º
        print(f"ã€€ã€€ã€€ã€€\033[32m{prompt_text}\033[0m")    # æ‰“å°è¿›å…¥ CLIP ç¼–ç æµç¨‹çš„æç¤ºè¯

        tokens = clip.tokenize(prompt_text)
        cond, pooled = clip.encode_from_tokens(tokens, return_pooled=True)

        return ([[cond, {"pooled_output": pooled}]], prompt_text)

    RETURN_TYPES = ("CONDITIONING", "STRING",)
    FUNCTION = "mtencode"
    CATEGORY = "MTCLIPEncode"

NODE_CLASS_MAPPINGS = {
    "MTCLIPEncode": MTCLIPEncode,
}

NODE_DISPLAY_NAME_MAPPINGS = {
    "MTCLIPEncode": "MTCLIPEncode",
}
